<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
</head>
<body>
<h1 id="finite-difference-practical-exercise">Finite Difference Practical Exercise</h1>
<p>This exercise extracts a highly common computational kernel seen across a wide variety of industries, a Laplacian calculation of a function sampled at regularly spaced intervals in 3D. This kernel can be the foundation for both a wave propagation solver (such as acoustic wave equation) or a diffusion equation solver (such as the Laplace equation, or heat equation). As such it is a commonly optimized routine.</p>
<p>This exercise will lead you on the first steps to optimizing this routine. So far you have learned the basics of parallelizing and vectorizing code using OpenMP. You have also learned some ways these tools can be misued to produce an incorrect answer. Here you will put these tools to use as well as learn some new features.</p>
<h2 id="suggestions-before-proceeding">Suggestions before proceeding</h2>
<p>In a production environment this work should be done under careful version control, but here to keep things simple it is recommended you keep a directory for experimenting and new directories for every new major change. This is so that you can keep track of performance gains or losses as you make changes to the code.</p>
<p>Another suggestion is to make sure anything that is parameterized be done so in a way that it can be easily changed. For example all parameters could be defined in a header file which is included in &quot;fd.cpp.&quot; This is useful especially if you ever do work which must perform well across architectures.</p>
<p>Finally: Make sure the test passes after each change to the code. The computed relative error should not exceed 1e-04 (roughly corresponding to four digits of accuracy).</p>
<h2 id="what-the-starting-code-comes-with">What the starting code comes with</h2>
<p>The starting code comes with the straightforward serial implementation of the stencil for you to optimize with parallelism and vectorization. It also contains a timer for this code, and finally it has a test routine to make sure the error is what it should. Note that error will be a little different with each change to code because floating point arithmetic is not associative. The important thing to guarantee here is that the error is sufficiently small - which for this application is around 1e-04 or 1e-05 (which due to the way I normalize the error, corresponds to about 4 or 5 digits of accuracy).</p>
<h2 id="step-1-parallelizing">Step 1: Parallelizing</h2>
<p>(solution found in &quot;step1&quot; directory)</p>
<h3 id="first-steps">First steps</h3>
<p>The code can be parallelized normally (by adding the appropriate pragma), but it might be worth exploring different options such as thread affinity and hyperthreading. To utilize hyperthreading, just set the num_threads() parameter to a larger number than the number of available cores. Generally speaking it is not usually worthwhile to try anything other than 1X threads per core, 2X threads per core, or 4X threads per core. Thread affinity can interact in interesting ways with hyperthreading as well, so I recommend trying many combinations.</p>
<h3 id="nested-parallelism-with-hot-teamsoptional">Nested Parallelism With Hot Teams(Optional)</h3>
<p>(solution found in &quot;step1_optional directory&quot;)</p>
<p>Instead of simply overprescribing the number of threads to achieve hyperthreading, you can also explicitly specify how the hyperthreads parallelize the code. For example you could do something like:</p>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="ot">#pragma omp parallel for num_threads(64) </span>
<span class="kw">for</span>(<span class="dt">int</span> i=<span class="dv">0</span>;i&lt;big;i++){
  <span class="ot">#pragma omp parallel for num_threads(4)</span>
  <span class="kw">for</span>(<span class="dt">int</span> j=<span class="dv">0</span>;j&lt;small;j++){
  <span class="co">// code here</span>
  }
}</code></pre></div>
<p>This can potentially give big gains. If you try this I recommend enabling Intel's &quot;hot teams&quot; mode which significantly lowers the overhead of the inner parallel loop. This can be be done in a shell script which executes the code</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"> <span class="co">#Enables Intel hot teams mode for efficient parallel inner loops.</span>
 <span class="kw">export</span> <span class="ot">KMP_HOT_TEAMS_MODE=</span>1
 <span class="co">#Sets the maximum nested-ness of parallelism to 2.</span>
 <span class="kw">export</span> <span class="ot">KMP_HOT_TEAMS_MAX_LEVEL=</span>2
 <span class="kw">./fd</span></code></pre></div>
<p>Another advantage of this approach is it gives much simpler control over affinity of the threads, since the outer threads can be forced to occupy whole cores while the inner threads forced to occupy only the associated hyperthreads.</p>
<h2 id="vectorization">Vectorization</h2>
<h3 id="step-2-vectorizing-inner-loop-as-is">Step 2: Vectorizing inner loop as-is</h3>
<p>(solution found in &quot;step2&quot; directory)</p>
<p>Vectorization of this code is where the interesting work happens. As a first approach to vectorizing it never hurts to just try the inner loop, which in this case is the loop over the stencil derivative calculation. This can be achieved by adding the &quot;omp simd&quot; pragma on the three inner loops.</p>
<p>Unfortunately this does not provide any performance gain. One problem is that the inner loop is just too complicated to easily vectorize, you can confirm this by outputting the vectorization report and seeing what sort of memory access patterns were assumed - in this case it should be gather/scatter because of the complex indexing,</p>
<p>Finally on the KNL we already mentioned it has an effective 32-float wide vector lane, but the stencil length (what we just vectorized over) is only 13 wide. We could try to combine all three derivative calculations into one, but that won't work either because their respective indexing are all different (unit stride in x direction, gather/scatter in y and z). This leads into a very common optimization tactic</p>
<h3 id="loop-reordering">Loop reordering</h3>
<p>(solution found in &quot;step3&quot; directory)</p>
<p>You may have noticed from the optimization report that the x-direction has unit stride access. It is very common in stencil calculations as well as n-dimensional array calculations to re-order for-loops so that the inner-most loop is looping over the fastest dimension, and then vectorize over this. This will solve the problem of complex indexing and should also fully occupy the vector lanes. Here is a simple example in C of this strategy:</p>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c">

<span class="dt">int</span> m=<span class="dv">32</span>;
<span class="dt">float</span> A[m][m][m];

<span class="kw">for</span>(<span class="dt">int</span> k=<span class="dv">0</span>;k&lt;m;k++)
  <span class="kw">for</span>(<span class="dt">int</span> i=<span class="dv">0</span>;i&lt;m;i++)
    <span class="kw">for</span>(<span class="dt">int</span> j=<span class="dv">0</span>;j&lt;m;j++)
      A[i][j][k]=<span class="fl">1.0</span>;</code></pre></div>
<p>which can be translated to the equivalent</p>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c">

<span class="dt">int</span> m=<span class="dv">32</span>;
<span class="dt">float</span> A[m][m][m];

<span class="kw">for</span>(<span class="dt">int</span> i=<span class="dv">0</span>;i&lt;m;i++)
  <span class="kw">for</span>(<span class="dt">int</span> j=<span class="dv">0</span>;j&lt;m;j++)
    <span class="kw">for</span>(<span class="dt">int</span> k=<span class="dv">0</span>;k&lt;m;k++)
      A[i][j][k]=<span class="fl">1.0</span>;</code></pre></div>
<p>and observe that because of C's storage order for arrays, the inner-most loop is always accessing contiguous data (which for vectorization translates to unit-stride access).</p>
<p>The code as provided for finite difference however is a little more &quot;realistic&quot; because it is also incorporating boundary conditions (or one possible way of doing so). When boundary conditions like this are included in the stencil computations, you usually have to separate out the conditions before reordering loops (or else possibly apply the wrong boundary conditions, which in a physics context usually results in wildly incorrect answers).</p>
<p>One way to achieve this would be to do what is called &quot;loop fission,&quot; this is where the loop is split into three identical loops each containing part of the original loop body. In this case we could split out the x,y, and z derivatives and this will allow loop reordering in the y and z dimensions. The x dimension is a little more complicated but it is already the fastest dimension so let's leave that for now and aim at the bigger cause of trouble.</p>
<p>Applying these optimizations the code should end up looking like the following</p>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c">
    <span class="kw">for</span>(<span class="dt">int64_t</span> iy=<span class="dv">0</span>;iy&lt;ny;iy++){
      <span class="dt">int64_t</span> jy=get_stencil(ny,iy);
      <span class="kw">for</span>(<span class="dt">int64_t</span> iz=<span class="dv">0</span>;iz&lt;nz;iz++){
        <span class="dt">int64_t</span> jz=get_stencil(nz,iz);
        <span class="co">/*Second order x-derivative.*/</span>
        <span class="kw">for</span>(<span class="dt">int64_t</span> ix=<span class="dv">0</span>;ix&lt;nx;ix++){ 
        <span class="dt">int64_t</span> jx=get_stencil(nx,ix);
          <span class="kw">for</span>(<span class="dt">int64_t</span> i=-jx;i&lt;=ORDER-jx ;i++){
              out[id(ix,iy,iz)] += coeffs[jx][i+jx]*in[id(ix+i,iy,iz)]/(dx*dx);
            }
        }
        <span class="co">/*Second order y-derivative.*/</span>
        <span class="kw">for</span>(<span class="dt">int64_t</span> i=-jy;i&lt;=ORDER-jy ;i++){
          <span class="kw">for</span>(<span class="dt">int64_t</span> ix=<span class="dv">0</span>;ix&lt;nx;ix++){ 
              out[id(ix,iy,iz)] += coeffs[jy][i+jy]*in[id(ix,iy+i,iz)]/(dy*dy);
            }
        }
        <span class="co">/*Second order z-derivative.*/</span>
        <span class="kw">for</span>(<span class="dt">int64_t</span> i=-jz;i&lt;=ORDER-jz ;i++){
          <span class="kw">for</span>(<span class="dt">int64_t</span> ix=<span class="dv">0</span>;ix&lt;nx;ix++){ 
            out[id(ix,iy,iz)] += coeffs[jz][i+jz]*in[id(ix,iy,iz+i)]/(dz*dz);
          }
        }
    }
  }</code></pre></div>
<p>This code combined several steps into one:</p>
<ul>
<li>Reordered loops to make the x direction the inner-most loop</li>
<li>Guaranteeing unit-stride indexing</li>
<li>Applied loop fission to separate the boundary logic for different dimensions</li>
</ul>
<p>This optimization is very portable, therefore it should yield the best performance on both KNL and non-KNL architectures, and we will see later that this holds true even without vectorization.</p>
<h2 id="summary-and-look-ahead">Summary and Look Ahead</h2>
<p>In this practical exercise we took a common computational kernel, a laplacian calculation for regularly spaced data, and parallelized and vectorized it. The parallelism was more or less straightforward except that some tunable parameters can be changed to yield better results. The vectorization required us to take a closer look at the code and modify it to be more amenable to using faster vector code.</p>
<p>These techniques are very important across a wide variety of situations, but significantly more performance is possible by further optimizing to make better use of memory - in a later practical exercise we will do this.</p>
</body>
</html>
